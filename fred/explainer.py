"""
FRED explainer

This class implements the FRED explainer algorithm for text classification.

FRED works by perturbing the input text and observing the change in the predicted output.
The words that have the biggest impact on the prediction are considered to be the most important
words for the explanation.
"""

import numpy as np
import itertools
from functools import reduce

from .explanation import Explanation
from fred.replacer import Replacer


class Fred:

    def __init__(self, classifier_fn, class_names=None, mask='UNK', pos=False):
        """
        Initializes a Fred explainer object.

        Args:
            class_names (list): A list of label names.
            classifier_fn (function): A function that takes in a text input and returns a probability distribution over classes.
            mask (str, optional): A string used to replace words during sampling. Defaults to 'UNK'.
            pos (bool, optional): A boolean flag indicating whether to use Word2Vec replacement during sampling. Defaults to False.
        """
        self.class_names = class_names
        self.classifier_fn = classifier_fn
        self.mask = mask
        self.pos = pos
        if self.pos:
            self.pos = Replacer()

    def generate_sample(self, example, max_len, perturb_proba=0.5, alpha=0.99):
        """
        Generates a new sample by perturbing a subset of words in the example.

        Args:
            example (str): The input text to be perturbed.
            max_len (int): The maximum length of the perturbed subsets of words.
            perturb_proba (float, optional): The probability of perturbing each word. Defaults to 0.5.
            alpha (float, optional): The confidence level for determining the number of samples to generate.
                                     Defaults to 0.99.

        Returns:
            list: A list of new samples generated by perturbing a subset of words in the example.
        """
        words = example.split()  # Split the example into a list of words
        b = len(words)  # Number of words in the example
        # Note:
        # if alpha=0.95 -> n_sample=3067
        # if alpha=0.99 -> n_sample=4714
        # if alpha=0.9999 -> n_sample=9427
        n_sample = int(
            np.ceil(np.log(1 - alpha) / np.log(1 - perturb_proba ** 10)))  # Number of samples to generate

        perturbed = np.empty((n_sample, b), dtype='U80')  # Array to store perturbed samples
        perturbed[:] = words  # Initialize perturbed samples with original words

        x = np.random.uniform(0, 1, size=(n_sample, b))  # Random values for each word in each sample
        print(f'x: {np.shape(x)} \n {x}')

        masked = np.where(x <= perturb_proba)  # Indices where the random values are less than or equal to perturb_proba

        # Apply perturbations to the masked indices by replacing the corresponding words with the word with the
        # highest cosine similarity
        if self.pos:
            for j in range(x.shape[1]):
                pert = masked[0][masked[1] == j]
                perturbed[pert, j] = self.pos.replace_word(words[j], len(pert))
        else:
            # Apply perturbations to the masked indices by replacing the corresponding words with a placeholder
            perturbed[masked] = self.mask

        sample = [' '.join(x) for x in perturbed]  # Convert the perturbed samples back to a list of strings

        word_removals = {j: [] for j in range(b)}  # Dictionary to store the removed word indices per word position

        for i, j in enumerate(masked[1]):
            word_removals[j].append(masked[0][i])  # Add the masked index to the corresponding word position

        return sample, word_removals

    def generate_candidates(self, example, size):
        """
        Generates all possible combinations of words with a given size from the example.

        Args:
            example (str): The input text, example to be explained.
            size (int): The number of words in each combination.

        Returns:
            tuple: A tuple containing a list of candidate combinations and a list of their corresponding indices.
        """

        # Split the example into a list of words
        words = example.split()

        # Generate indices combinations
        comb_indices = itertools.combinations(range(len(words)), size)

        # Convert combinations to lists of indices
        candidate_idx = [[i for i in comb] for comb in comb_indices]

        # Generate word combinations
        candidates = [list(comb) for comb in itertools.combinations(words, size)]

        return candidates, candidate_idx

    def beam_candidates(self, candidates_ids, word_drops, words, beam_size):
        """
        Generates the next beam of candidates using a beam search algorithm.

        Args:
            candidates_ids (list): A list of indices for the candidate combinations of words.
            word_drops (dict): A dictionary containing word drops for each word index.
            words (list): A list of all words in the example.
            beam_size (int): The beam size.

        Returns:
            tuple: A tuple containing a list of candidate combinations and a list of their corresponding indices.
        """

        # Convert the 'words' list to a NumPy array for efficient operations
        words = np.array(words)

        # Get individual words ids sorted by importance
        word_ids = list(dict(sorted(word_drops.items(), key=lambda item: item[1], reverse=True)).keys())

        # Select the top 'beam_size' candidates
        prev_candidates_ids = np.array(candidates_ids)[:beam_size]

        # Initialize the new list of candidates and their corresponding indices
        candidates = []
        new_candidates_ids = []

        # Iterate over the previous candidates
        for candidate in prev_candidates_ids:

            # Find missing elements not present in the candidate
            missing_elements = np.setdiff1d(word_ids, candidate)

            # If there are missing elements
            if len(missing_elements) > 0:
                # Replicate the candidate list for each missing element
                new_lists = np.tile(candidate, (len(missing_elements), 1))

                # Append the missing element to each replicated list
                new_lists = np.hstack((new_lists, missing_elements[:, np.newaxis]))

                # Extend the new_candidates_ids list with the new candidates
                new_candidates_ids.extend(new_lists)

                # Convert the new_candidates_ids to candidate words
                candidates.extend([words[i].tolist() for i in new_lists])

        # Return the new list of candidates and their corresponding indices
        return candidates, new_candidates_ids

    def compute_drop(self, candidates_ids, sample_drops, word_removals):
        """
        Computes the drop in confidence for a candidate combination of words.

        Args:
            candidates_ids (list): A list of indices for the candidate combination of words.
            sample_drops (list): A list of confidence drops for each perturbed example.
            word_removals (dict): A dictionary containing word removals for each word index.

        Returns:
            float: The average drop in confidence for the candidate combination of words.
        """

        # Find the common removed words
        sample_c = reduce(np.intersect1d, [word_removals[k] for k in candidates_ids])

        # Retrieve the confidence drops for the common removed words
        drops = [sample_drops[i] for i in sample_c]

        # Compute the average drop in confidence
        return np.mean(drops)

    def explain_instance(self, example, eps=0.15, max_len=10, beam_size=4, label=None, verbose=False):
        """
        Generates an explanation for a given input text.

        Args:
            example (str): The input text to be explained.
            eps (float, optional): The threshold for the drop in confidence to determine the best subset of words.
                                       Defaults to 0.15.
            max_len (int, optional): The maximum length of the perturbed subsets of words. Defaults to 10.
            label (int, optional): The label to explain. If None, the predicted label.
            beam_size (int, optional): beam size
            verbose (bool, optional): if print info

        Returns:
            Explanation: An Explanation object that contains the best subset of words and its drop in confidence.
        """

        # Split the example into a list of words
        words = example.split()

        if label is None:
            label = np.argmax(self.classifier_fn([example]))
        # Compute the confidence score of the example prediction
        confidence = self.classifier_fn([example])[:, label]

        # Initialize a dictionary to store the drop in confidence for each word
        word_drops = {}

        # Generate perturbed samples and compute the drop in confidence for each sample
        sample, word_removals = self.generate_sample(example, max_len=max_len)
        preds = self.classifier_fn(sample)[:, label]
        sample_drops = confidence - np.array(preds)

        # Initialize the best subset of words and its drop in confidence
        best = []
        best_drop = 0
        best_ids = []
        best_drops_sum = 0

        # Initialize a dictionary to store the drop in confidence for each candidate combination of words
        candidate_drops = {}

        # Initialize the list of candidate combinations of words and their corresponding indices
        candidates = [[w] for w in words]
        candidates_ids = [[k] for k in range(len(words))]

        # Iterate over the maximum length of the candidate combinations of words
        for size in range(1, max_len + 1):

            # If there are candidate combinations of words with the current size
            if candidate_drops:
                # Sort the candidate combinations of words by their drop in confidence
                candidate_drops = dict(sorted(candidate_drops.items(), key=lambda item: item[1], reverse=True))
                sorted_ids = list(candidate_drops.keys())
                sorted_candidates_ids = np.array(candidates_ids)[sorted_ids]

                # Update the list of candidate combinations of words and their corresponding indices
                candidates, candidates_ids = self.beam_candidates(sorted_candidates_ids, word_drops, words, beam_size)

            # Reset the dictionary to store the drop in confidence for each candidate combination of words
            candidate_drops = {}
            # Iterate over the candidate combinations of words with the current size
            for i, candidate in enumerate(candidates):
                # Compute the drop in confidence for the current candidate combination of words
                candidate_drops[i] = self.compute_drop(candidates_ids[i], sample_drops, word_removals)

                # If the current candidate combination of words is a single word
                if len(candidate) == 1:
                    # Update the dictionary to store the drop in confidence for each word
                    word_drops[i] = candidate_drops[i]

                # If the drop in confidence for the current candidate combination of words is greater than the best
                # drop in confidence
                if candidate_drops[i] > best_drop:

                    # Update the best subset of words and its drop in confidence
                    best = candidate
                    best_drop = candidate_drops[i]
                    best_ids = candidates_ids[i]

                    # Compute the sum of the drop in confidence for the individual words in the best subset of words
                    best_drops_sum = np.sum([word_drops[k] for k in candidates_ids[i]])

                # If the drop in confidence for the current candidate combination of words is equal to the best drop
                # in confidence
                elif candidate_drops[i] == best_drop:

                    # Compute the sum of the drop in confidence for the individual words in the current candidate
                    # combination of words
                    drops_sum = np.sum([word_drops[k] for k in candidates_ids[i]])

                    # If the sum of the drop in confidence for the individual words in the current candidate
                    # combination of words is greater than or equal to the sum of the drop in confidence for the
                    # individual words in the best subset of words
                    if drops_sum >= best_drops_sum:
                        # Update the best subset of words and its drop in confidence
                        best = candidate
                        best_drop = candidate_drops[i]
                        best_ids = candidates_ids[i]

                # If the drop in confidence for the best subset of words is greater than or equal to the threshold
            if best_drop >= eps * confidence:
                # Return an Explanation object that contains the best subset of words and its drop in confidence
                explanation = Explanation(words, best, best_drop, best_ids, word_drops, label, self.class_names, verbose)
                return explanation

        # If the drop in confidence for the best subset of words is less than the threshold
        # Return an Explanation object that contains the best subset of words and its drop in confidence
        explanation = Explanation(words, best, best_drop, best_ids, word_drops, label, self.class_names, verbose)
        return explanation
